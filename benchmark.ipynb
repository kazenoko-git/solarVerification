{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2575a94b",
   "metadata": {},
   "source": [
    "# Terralyte Model Benchmarking Suite\n",
    "\n",
    "Relative + Absolute Benchmark Notebook  \n",
    "Compares `verifier1.pt` vs `verifier2.pt`, and evaluates against real ground truth.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2652ffc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# IMPORTS\n",
    "\n",
    "import os\n",
    "import csv\n",
    "import json\n",
    "import math\n",
    "import subprocess\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    confusion_matrix,\n",
    "    mean_absolute_error\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c371c40e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONFIGURATION\n",
    "\n",
    "PY = \"/usr/local/bin/python3\"  \n",
    "\n",
    "MODEL1 = \"verifier1.pt\"\n",
    "MODEL2 = \"verifier2.pt\"\n",
    "\n",
    "# CSVs\n",
    "RELATIVE_CSV = \"rel_benchmark.csv\"      # sample_id,lat,lon\n",
    "ABSOLUTE_CSV = \"abs_benchmark.csv\"       # sample_id,lat,lon,has_solar,panel_count\n",
    "\n",
    "print(\"Using Python:\", PY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1541e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HELPERS \n",
    "def run_tile_download(lat, lon, zoom=18, radius=1):\n",
    "    \"\"\"Runs imagenRunner.py and returns tile PNG path.\"\"\"\n",
    "    p = subprocess.run(\n",
    "        [PY, \"imagenRunner.py\", str(lat), str(lon), str(zoom), str(radius), \"esri\", \"--crop\"],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if p.returncode != 0:\n",
    "        raise RuntimeError(p.stderr)\n",
    "    return p.stdout.strip().split(\"\\n\")[-1]\n",
    "\n",
    "\n",
    "def run_model(image_path, model_path):\n",
    "    \"\"\"Runs YOLO model via run_model.py and returns JSON.\"\"\"\n",
    "    p = subprocess.run(\n",
    "        [PY, \"run_model.py\", image_path, model_path],\n",
    "        capture_output=True,\n",
    "        text=True\n",
    "    )\n",
    "    if p.returncode != 0:\n",
    "        raise RuntimeError(p.stderr)\n",
    "    return json.loads(p.stdout.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2404008",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELATIVE BENCHMARKING\n",
    "\n",
    "def relative_benchmark(csv_path):\n",
    "    ids, m1_labels, m2_labels = [], [], []\n",
    "    m1_counts, m2_counts = [], []\n",
    "\n",
    "    with open(csv_path, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        for r in reader:\n",
    "            sid = r[\"sample_id\"]\n",
    "            lat, lon = float(r[\"lat\"]), float(r[\"lon\"])\n",
    "\n",
    "            print(f\"→ {sid} ({lat}, {lon})\")\n",
    "            img = run_tile_download(lat, lon)\n",
    "\n",
    "            r1 = run_model(img, MODEL1)\n",
    "            r2 = run_model(img, MODEL2)\n",
    "\n",
    "            ids.append(sid)\n",
    "            m1_labels.append(1 if r1[\"has_solar\"] else 0)\n",
    "            m2_labels.append(1 if r2[\"has_solar\"] else 0)\n",
    "\n",
    "            m1_counts.append(r1[\"panel_count_est\"])\n",
    "            m2_counts.append(r2[\"panel_count_est\"])\n",
    "\n",
    "    return {\n",
    "        \"ids\": ids,\n",
    "        \"m1_labels\": m1_labels,\n",
    "        \"m2_labels\": m2_labels,\n",
    "        \"m1_counts\": m1_counts,\n",
    "        \"m2_counts\": m2_counts,\n",
    "    }\n",
    "\n",
    "# RUN RELATIVE BENCHMARK\n",
    "\n",
    "rel = relative_benchmark(RELATIVE_CSV)\n",
    "print(\"\\nRelative Benchmark Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "064dca60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RELATIVE METRICS\n",
    "\n",
    "def relative_metrics(pred1, pred2):\n",
    "    acc = accuracy_score(pred1, pred2)\n",
    "    prec = precision_score(pred1, pred2)\n",
    "    rec = recall_score(pred1, pred2)\n",
    "    f1 = f1_score(pred1, pred2)\n",
    "    return acc, prec, rec, f1\n",
    "\n",
    "rel_metric = relative_metrics(rel[\"m1_labels\"], rel[\"m2_labels\"])\n",
    "\n",
    "print(\"Relative Model2 vs Model1:\")\n",
    "print(\"Accuracy :\", rel_metric[0])\n",
    "print(\"Precision:\", rel_metric[1])\n",
    "print(\"Recall   :\", rel_metric[2])\n",
    "print(\"F1 Score :\", rel_metric[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c87a0da5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'csv' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 43\u001b[0m\n\u001b[1;32m     32\u001b[0m             m2_counts\u001b[38;5;241m.\u001b[39mappend(r2[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpanel_count_est\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m     35\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt_labels\u001b[39m\u001b[38;5;124m\"\u001b[39m: gt_labels,\n\u001b[1;32m     36\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mgt_counts\u001b[39m\u001b[38;5;124m\"\u001b[39m: gt_counts,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mm2_counts\u001b[39m\u001b[38;5;124m\"\u001b[39m: m2_counts,\n\u001b[1;32m     41\u001b[0m     }\n\u001b[0;32m---> 43\u001b[0m absb \u001b[38;5;241m=\u001b[39m \u001b[43mabsolute_benchmark\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mabs_benchmark.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mAbsolute Benchmark Complete!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[0;32mIn[1], line 9\u001b[0m, in \u001b[0;36mabsolute_benchmark\u001b[0;34m(csv_path)\u001b[0m\n\u001b[1;32m      6\u001b[0m m2_labels, m2_counts \u001b[38;5;241m=\u001b[39m [], []\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(csv_path, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m----> 9\u001b[0m     reader \u001b[38;5;241m=\u001b[39m \u001b[43mcsv\u001b[49m\u001b[38;5;241m.\u001b[39mDictReader(f)\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m reader:\n\u001b[1;32m     12\u001b[0m         sid \u001b[38;5;241m=\u001b[39m r[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msample_id\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'csv' is not defined"
     ]
    }
   ],
   "source": [
    "# ABSOLUTE BENCHMARKING\n",
    "\n",
    "def absolute_benchmark(csv_path):\n",
    "    gt_labels, gt_counts = [], []\n",
    "    m1_labels, m1_counts = [], []\n",
    "    m2_labels, m2_counts = [], []\n",
    "\n",
    "    with open(csv_path, \"r\") as f:\n",
    "        reader = csv.DictReader(f)\n",
    "\n",
    "        for r in reader:\n",
    "            sid = r[\"sample_id\"]\n",
    "            lat = float(r[\"lat\"])\n",
    "            lon = float(r[\"lon\"])\n",
    "            gt_has = int(r[\"has_solar\"])\n",
    "            gt_cnt = int(r[\"panel_count\"])\n",
    "\n",
    "            print(f\"→ {sid} ({lat}, {lon})\")\n",
    "\n",
    "            img = run_tile_download(lat, lon)\n",
    "\n",
    "            r1 = run_model(img, MODEL1)\n",
    "            r2 = run_model(img, MODEL2)\n",
    "\n",
    "            gt_labels.append(gt_has)\n",
    "            gt_counts.append(gt_cnt)\n",
    "\n",
    "            m1_labels.append(1 if r1[\"has_solar\"] else 0)\n",
    "            m1_counts.append(r1[\"panel_count_est\"])\n",
    "\n",
    "            m2_labels.append(1 if r2[\"has_solar\"] else 0)\n",
    "            m2_counts.append(r2[\"panel_count_est\"])\n",
    "\n",
    "    return {\n",
    "        \"gt_labels\": gt_labels,\n",
    "        \"gt_counts\": gt_counts,\n",
    "        \"m1_labels\": m1_labels,\n",
    "        \"m1_counts\": m1_counts,\n",
    "        \"m2_labels\": m2_labels,\n",
    "        \"m2_counts\": m2_counts,\n",
    "    }\n",
    "\n",
    "absb = absolute_benchmark(ABSOLUTE_CSV)\n",
    "print(\"\\nAbsolute Benchmark Complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43734dfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ABSOLUTE METRICS\n",
    "\n",
    "def compute_abs(gt, pred, gt_cnt, pred_cnt):\n",
    "    acc = accuracy_score(gt, pred)\n",
    "    prec = precision_score(gt, pred, zero_division=0)\n",
    "    rec = recall_score(gt, pred, zero_division=0)\n",
    "    f1 = f1_score(gt, pred, zero_division=0)\n",
    "\n",
    "    mae = mean_absolute_error(gt_cnt, pred_cnt)\n",
    "    rmse = math.sqrt(sum((a - b)**2 for a, b in zip(gt_cnt, pred_cnt)) / len(gt_cnt))\n",
    "\n",
    "    return acc, prec, rec, f1, mae, rmse\n",
    "\n",
    "\n",
    "m1_abs = compute_abs(absb[\"gt_labels\"], absb[\"m1_labels\"], absb[\"gt_counts\"], absb[\"m1_counts\"])\n",
    "m2_abs = compute_abs(absb[\"gt_labels\"], absb[\"m2_labels\"], absb[\"gt_counts\"], absb[\"m2_counts\"])\n",
    "\n",
    "print(\"Model 1 Absolute:\", m1_abs)\n",
    "print(\"Model 2 Absolute:\", m2_abs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc8dfb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PLOT\n",
    "\n",
    "plt.figure(figsize=(7,7))\n",
    "plt.scatter(absb[\"gt_counts\"], absb[\"m1_counts\"], label=\"Model1\", alpha=0.7)\n",
    "plt.scatter(absb[\"gt_counts\"], absb[\"m2_counts\"], label=\"Model2\", alpha=0.7)\n",
    "plt.plot([0, max(absb[\"gt_counts\"])], [0, max(absb[\"gt_counts\"])], 'k--', label=\"Ideal\")\n",
    "\n",
    "plt.xlabel(\"Ground Truth Count\")\n",
    "plt.ylabel(\"Predicted Count\")\n",
    "plt.legend()\n",
    "plt.title(\"Absolute Count Prediction Accuracy\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
